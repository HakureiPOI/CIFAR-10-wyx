{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPSiy36GHzx1Za96Tupj/LA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HakureiPOI/CIFAR-10-wyx/blob/main/cifar10_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 基于 CIFAR10 和 ResNet18 的图像分类"
      ],
      "metadata": {
        "id": "9piTDlA2ETbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "z4fNdf4c2PPt",
        "outputId": "3d8df252-a540-43c6-d39d-4dda58f53d36"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Part1. 获取 CIFAR-10 数据集"
      ],
      "metadata": {
        "id": "D_WtcZYfH14m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "bDGo_WYvFqkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WsW4HyXD7C8"
      },
      "outputs": [],
      "source": [
        "# 获取 CIFAR-10 数据\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 是一个十分类的图像数据集\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "21gAXznoFFLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]      #  32 × 32 的 3通道图片，各"
      ],
      "metadata": {
        "id": "phFE-9uyGm3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]      # 标签是 0~9 十个离散整数值"
      ],
      "metadata": {
        "id": "jiJCDow3HC5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5jgZIcKkFiKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "images_idx = [] # 记录一下这里展示过的图片\n",
        "\n",
        "# 随机展示一下各类别的图片\n",
        "for i in range(len(class_names)):\n",
        "    class_indices = np.where(y_train == i)[0]\n",
        "    idx = np.random.choice(class_indices)\n",
        "    images_idx.append(idx)\n",
        "\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(X_train[idx])\n",
        "    plt.title(class_names[i])\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Nub0K-nFrsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Part2. 对影像数据进行预处理"
      ],
      "metadata": {
        "id": "scjiKTl-H7sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image, label):\n",
        "    # 像素值归一化\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    # 随机翻转\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    # image = tf.image.random_flip_up_down(image)  # 可选上下翻转\n",
        "\n",
        "    # 50% 概率高斯模糊\n",
        "    def get_gaussian_kernel(kernel_size, sigma, n_channels):\n",
        "        # 生成 1D 高斯\n",
        "        x = tf.range(-(kernel_size // 2), kernel_size // 2 + 1, dtype=tf.float32)\n",
        "        x = tf.exp(-(x**2) / (2.0 * sigma**2))\n",
        "        x = x / tf.reduce_sum(x)\n",
        "\n",
        "        # 外积得到 2D 高斯核 [k,k]\n",
        "        kernel2d = tf.tensordot(x, x, axes=0)\n",
        "        kernel2d = kernel2d[:, :, tf.newaxis, tf.newaxis]          # [k,k,1,1]\n",
        "        kernel2d = tf.repeat(kernel2d, n_channels, axis=2)         # [k,k,C,1]\n",
        "        return kernel2d\n",
        "\n",
        "    def blur():\n",
        "        kernel_size = 3\n",
        "        sigma = tf.random.uniform((), minval=0.5, maxval=1.5)\n",
        "        kernel = get_gaussian_kernel(kernel_size, sigma, tf.shape(image)[-1])\n",
        "        return tf.nn.depthwise_conv2d(image[tf.newaxis, ...], kernel,\n",
        "                                      strides=[1, 1, 1, 1], padding='SAME')[0]\n",
        "\n",
        "    image = tf.cond(tf.random.uniform(()) > 0.5, blur, lambda: image)\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "IH9XHeweFn2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 将之前展示的图片预处理后查看效果\n",
        "plt.figure(figsize = (12, 6))\n",
        "\n",
        "for i, idx in enumerate(images_idx):\n",
        "    image, label = preprocess_image(X_train[idx], y_train[idx])\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(image.numpy())\n",
        "    plt.title(class_names[label.item()])\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tp2IdQtvMD-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 构建数据集并应用预处理函数\n",
        "batch_size = 128\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Fa6WGlsQNJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image, label in train_dataset.take(1):\n",
        "    idx = np.random.choice(image.shape[0])\n",
        "\n",
        "    plt.figure(figsize = (4, 4))\n",
        "    plt.imshow(image[idx].numpy())\n",
        "    plt.title(class_names[label[idx].numpy().item()])\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "EIaIZFEbO3Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Part3. 搭建网络进行训练"
      ],
      "metadata": {
        "id": "su4uvz1-QfdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models, regularizers"
      ],
      "metadata": {
        "id": "AKEhFSHVTCEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet18_cifar10(\n",
        "    input_shape = (32, 32, 3),\n",
        "    num_classes = 10,\n",
        "    base_width = 64,\n",
        "    weight_decay = 5e-4,\n",
        "    dropout_rate = 0.0,\n",
        "):\n",
        "\n",
        "    wd = regularizers.l2(weight_decay)\n",
        "\n",
        "    def conv3x3(filters, stride=1):\n",
        "        # 3x3 卷积\n",
        "        return layers.Conv2D(\n",
        "            filters, 3, strides=stride, padding='same',\n",
        "            use_bias=False, kernel_initializer='he_normal',\n",
        "            kernel_regularizer=wd\n",
        "        )\n",
        "\n",
        "    def conv1x1(filters, stride = 1):\n",
        "        # 1x1 卷积：用于 shortcut 对齐通道/下采样\n",
        "        return layers.Conv2D(\n",
        "            filters, 1, strides=stride, padding='valid',\n",
        "            use_bias=False, kernel_initializer='he_normal',\n",
        "            kernel_regularizer=wd\n",
        "        )\n",
        "\n",
        "    def preact_basic_block(x, filters, stride = 1):\n",
        "        shortcut = x\n",
        "\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        # 如果需要下采样/通道对齐：用 1x1 conv 处理 shortcut\n",
        "        if stride != 1 or shortcut.shape[-1] != filters:\n",
        "            shortcut = conv1x1(filters, stride=stride)(x)\n",
        "\n",
        "        # 主分支两层 3x3\n",
        "        x = conv3x3(filters, stride=stride)(x)\n",
        "\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        if dropout_rate > 0:\n",
        "            x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "        x = conv3x3(filters, stride = 1)(x)\n",
        "\n",
        "        # 残差相加\n",
        "        x = layers.Add()([x, shortcut])\n",
        "        return x\n",
        "\n",
        "    def make_stage(x, filters, blocks, first_stride):\n",
        "        # 一个 stage：第一个 block 可能 stride = 2 下采样，其余 stride = 1\n",
        "        x = preact_basic_block(x, filters, stride=first_stride)\n",
        "        for _ in range(1, blocks):\n",
        "            x = preact_basic_block(x, filters, stride=1)\n",
        "        return x\n",
        "\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # CIFAR 的 stem 用 3x3 stride = 1，避免过早下采样\n",
        "    x = layers.Conv2D(\n",
        "        base_width, 3, strides=1, padding='same',\n",
        "        use_bias=False, kernel_initializer='he_normal',\n",
        "        kernel_regularizer=wd\n",
        "    )(inputs)\n",
        "\n",
        "    # ResNet-18: (2,2,2,2)\n",
        "    x = make_stage(x, base_width,     blocks=2, first_stride=1)  # 32x32\n",
        "    x = make_stage(x, base_width*2,   blocks=2, first_stride=2)  # 16x16\n",
        "    x = make_stage(x, base_width*4,   blocks=2, first_stride=2)  # 8x8\n",
        "    x = make_stage(x, base_width*8,   blocks=2, first_stride=2)  # 4x4\n",
        "\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = layers.Dense(\n",
        "        num_classes, activation='softmax',\n",
        "        kernel_regularizer=wd\n",
        "    )(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs, name='ResNet18_CIFAR10')\n",
        "    return model"
      ],
      "metadata": {
        "id": "79nVFd_GQqSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18_cifar10(input_shape=(32,32,3), num_classes=10, weight_decay=5e-4, dropout_rate=0.3)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "i5wPJ7UdTXCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers, callbacks"
      ],
      "metadata": {
        "id": "ivfb3eKgVTFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "\n",
        "steps_per_epoch = len(X_train) // batch_size\n",
        "total_steps = steps_per_epoch * epochs\n",
        "\n",
        "# 模拟退火\n",
        "lr_schedule = optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=3e-3,\n",
        "    decay_steps=total_steps,\n",
        "    alpha=1e-2\n",
        ")\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# 早停机制\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    patience=15,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 保存最好模型\n",
        "ckpt = callbacks.ModelCheckpoint(\n",
        "    'best_cifar10_adam.keras',\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "9tT2__KHVylU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[ckpt, early_stop]\n",
        ")"
      ],
      "metadata": {
        "id": "S2ivLu29W8l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/cifar10_models/resnet18_best.keras\"\n",
        "\n",
        "import os\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "model.save(save_path)\n",
        "print(\"Saved to:\", save_path)"
      ],
      "metadata": {
        "id": "jm41kxVRyvnC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}